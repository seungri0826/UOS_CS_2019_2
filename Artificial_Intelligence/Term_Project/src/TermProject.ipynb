{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "step2_2수정-02-994900.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c_X2qg16sLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "np.random.seed(20160704)\n",
        "tf.set_random_seed(20160704)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb0AtDRg6uoc",
        "colab_type": "code",
        "outputId": "c0e832a5-85b1-472d-dfbc-6e0df0e441eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QzGCn7TVMl5",
        "colab_type": "code",
        "outputId": "910a9e73-f442-45e3-954f-69b06adfb011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\"\"\"\n",
        "-batch size: 32\n",
        "-FC node #: 512\n",
        "-filter #: 64/128/128 \n",
        "-activation function(hidden cutoff): relu  \n",
        "-keep_prob: 0.7\n",
        "-b_conv: 0.1\n",
        "-activation(FC): relu\n",
        "-learning rate: 0.0001\n",
        "\"\"\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n-batch size: 32\\n-FC node #: 512\\n-filter #: 64/128/128 \\n-activation function(hidden cutoff): relu  \\n-keep_prob: 0.7\\n-b_conv: 0.1\\n-activation(FC): relu\\n-learning rate: 0.0001\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN0Fnn_z6zsS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_filters1 = 64\n",
        "\n",
        "x = tf.placeholder(tf.float32, [None, 784])\n",
        "x_image = tf.reshape(x, [-1,28,28,1])\n",
        "\n",
        "W_conv1 = tf.Variable(tf.truncated_normal([3,3,1,num_filters1], # filter size 3X3\n",
        "                                          stddev=0.1))\n",
        "h_conv1 = tf.nn.conv2d(x_image, W_conv1,\n",
        "                       strides=[1,1,1,1], padding='SAME')\n",
        "\n",
        "b_conv1 = tf.Variable(tf.constant(0.1, shape=[num_filters1]))\n",
        "h_conv1_cutoff = tf.nn.relu(h_conv1 + b_conv1)\n",
        "\n",
        "h_pool1 = tf.nn.max_pool(h_conv1_cutoff, ksize=[1,2,2,1],\n",
        "                         strides=[1,2,2,1], padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-DCpmCv62wu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_filters2 = 128\n",
        "\n",
        "W_conv2 = tf.Variable(\n",
        "            tf.truncated_normal([3,3,num_filters1,num_filters2], # filter size 3X3\n",
        "                                stddev=0.1))\n",
        "h_conv2 = tf.nn.conv2d(h_pool1, W_conv2,\n",
        "                       strides=[1,1,1,1], padding='SAME')\n",
        "\n",
        "b_conv2 = tf.Variable(tf.constant(0.1, shape=[num_filters2]))\n",
        "h_conv2_cutoff = tf.nn.relu(h_conv2 + b_conv2)\n",
        "\n",
        "h_pool2 = tf.nn.max_pool(h_conv2_cutoff, ksize=[1,2,2,1],\n",
        "                         strides=[1,2,2,1], padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJom8Rw2B8g2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# additional hidden layer\n",
        "num_filters3 = 128\n",
        "\n",
        "W_conv3 = tf.Variable(\n",
        "            tf.truncated_normal([3,3,num_filters2,num_filters3], # filter size 3X3\n",
        "                                stddev=0.1))\n",
        "h_conv3 = tf.nn.conv2d(h_pool2, W_conv3,\n",
        "                       strides=[1,1,1,1], padding='SAME')\n",
        "\n",
        "b_conv3 = tf.Variable(tf.constant(0.1, shape=[num_filters3]))\n",
        "h_conv3_cutoff = tf.nn.relu(h_conv3 + b_conv3)\n",
        "\n",
        "h_pool3 = tf.nn.max_pool(h_conv3_cutoff, ksize=[1,2,2,1],\n",
        "                         strides=[1,2,2,1], padding='SAME')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1ltfHV6CuoB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fully connected layer, dropout layer, softmax function\n",
        "h_pool3_flat = tf.reshape(h_pool3, [-1, 4*4*num_filters3]) \n",
        "\n",
        "num_units1 = 4*4*num_filters3 # fully connected layer에 입력할 데이터 개수\n",
        "num_units2 = 512 # fully connected layer의 node 개수\n",
        "\n",
        "w2 = tf.Variable(tf.truncated_normal([num_units1, num_units2]))\n",
        "b2 = tf.Variable(tf.constant(0.1, shape=[num_units2]))\n",
        "hidden2 = tf.nn.relu(tf.matmul(h_pool3_flat, w2) + b2)\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32) # dropout probability\n",
        "hidden3_drop = tf.nn.dropout(hidden2, keep_prob)\n",
        "\n",
        "w0 = tf.Variable(tf.zeros([num_units2, 10]))\n",
        "b0 = tf.Variable(tf.zeros([10]))\n",
        "p = tf.nn.softmax(tf.matmul(hidden3_drop, w0) + b0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y_yqJOV677q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = tf.placeholder(tf.float32, [None, 10])\n",
        "loss = -tf.reduce_sum(t * tf.log(tf.clip_by_value(p, 1e-10, 1.0)))\n",
        "train_step = tf.train.AdamOptimizer(0.0001).minimize(loss)\n",
        "correct_prediction = tf.equal(tf.argmax(p, 1), tf.argmax(t, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pv-4lzB6-eN",
        "colab_type": "code",
        "outputId": "27f5bbe3-de7d-4018-f11c-98977a3a50e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "sess = tf.InteractiveSession()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "saver = tf.train.Saver()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDpt3sbF7Aig",
        "colab_type": "code",
        "outputId": "35826f50-8ecb-404d-9efa-e741e653ac92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "i = 0\n",
        "for _ in range(200000):\n",
        "    i += 1\n",
        "    batch_xs, batch_ts = mnist.train.next_batch(32) # 신경망이 복잡해질수록 작은 batch size\n",
        "    sess.run(train_step,\n",
        "             feed_dict={x:batch_xs, t:batch_ts, keep_prob:0.7}) # training 시 parameter 최적화\n",
        "    if i % 500 == 0:\n",
        "        loss_vals, acc_vals = [], []\n",
        "        for c in range(4):\n",
        "            start = len(mnist.test.labels) // 4 * c\n",
        "            end = len(mnist.test.labels) // 4 * (c+1)\n",
        "            loss_val, acc_val = sess.run([loss, accuracy],\n",
        "                feed_dict={x:mnist.test.images[start:end],\n",
        "                           t:mnist.test.labels[start:end],\n",
        "                           keep_prob:1.0})  # parameter 최적화 완료 후 미지의 데이터에 대한 예측할 때 (test 시에는 모두 사용)\n",
        "            loss_vals.append(loss_val)\n",
        "            acc_vals.append(acc_val)\n",
        "        loss_val = np.sum(loss_vals)\n",
        "        acc_val = np.mean(acc_vals)\n",
        "        print ('Step: %d, Loss: %f, Accuracy: %f'\n",
        "               % (i, loss_val, acc_val))\n",
        "        saver.save(sess, 'cnn_session', global_step=i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step: 500, Loss: 1991.633057, Accuracy: 0.938800\n",
            "Step: 1000, Loss: 1121.740601, Accuracy: 0.966400\n",
            "Step: 1500, Loss: 932.853638, Accuracy: 0.971300\n",
            "Step: 2000, Loss: 785.648743, Accuracy: 0.975600\n",
            "Step: 2500, Loss: 706.690125, Accuracy: 0.976400\n",
            "Step: 3000, Loss: 558.167603, Accuracy: 0.980600\n",
            "Step: 3500, Loss: 529.319336, Accuracy: 0.981700\n",
            "Step: 4000, Loss: 563.638794, Accuracy: 0.981500\n",
            "Step: 4500, Loss: 538.173035, Accuracy: 0.981100\n",
            "Step: 5000, Loss: 610.127686, Accuracy: 0.979200\n",
            "Step: 5500, Loss: 435.879700, Accuracy: 0.986100\n",
            "Step: 6000, Loss: 397.253510, Accuracy: 0.986300\n",
            "Step: 6500, Loss: 353.763672, Accuracy: 0.987200\n",
            "Step: 7000, Loss: 472.455078, Accuracy: 0.984600\n",
            "Step: 7500, Loss: 496.576508, Accuracy: 0.984400\n",
            "Step: 8000, Loss: 418.347137, Accuracy: 0.986500\n",
            "Step: 8500, Loss: 362.044556, Accuracy: 0.986400\n",
            "Step: 9000, Loss: 321.900024, Accuracy: 0.988900\n",
            "Step: 9500, Loss: 353.719299, Accuracy: 0.988400\n",
            "Step: 10000, Loss: 342.230286, Accuracy: 0.988400\n",
            "Step: 10500, Loss: 338.214783, Accuracy: 0.989700\n",
            "Step: 11000, Loss: 380.206451, Accuracy: 0.987300\n",
            "Step: 11500, Loss: 374.965515, Accuracy: 0.987600\n",
            "Step: 12000, Loss: 318.626099, Accuracy: 0.989300\n",
            "Step: 12500, Loss: 323.861084, Accuracy: 0.989500\n",
            "Step: 13000, Loss: 313.045288, Accuracy: 0.989200\n",
            "Step: 13500, Loss: 321.225525, Accuracy: 0.988900\n",
            "Step: 14000, Loss: 310.817963, Accuracy: 0.989900\n",
            "Step: 14500, Loss: 339.222443, Accuracy: 0.989200\n",
            "Step: 15000, Loss: 316.427185, Accuracy: 0.989800\n",
            "Step: 15500, Loss: 293.908661, Accuracy: 0.990100\n",
            "Step: 16000, Loss: 307.914978, Accuracy: 0.989900\n",
            "Step: 16500, Loss: 311.652039, Accuracy: 0.990000\n",
            "Step: 17000, Loss: 286.265686, Accuracy: 0.990300\n",
            "Step: 17500, Loss: 306.565887, Accuracy: 0.991500\n",
            "Step: 18000, Loss: 315.835602, Accuracy: 0.990900\n",
            "Step: 18500, Loss: 270.187592, Accuracy: 0.990900\n",
            "Step: 19000, Loss: 270.626007, Accuracy: 0.992400\n",
            "Step: 19500, Loss: 303.611694, Accuracy: 0.990300\n",
            "Step: 20000, Loss: 282.726440, Accuracy: 0.990500\n",
            "Step: 20500, Loss: 311.045349, Accuracy: 0.990200\n",
            "Step: 21000, Loss: 265.724548, Accuracy: 0.991400\n",
            "Step: 21500, Loss: 264.510742, Accuracy: 0.991300\n",
            "Step: 22000, Loss: 284.228485, Accuracy: 0.990900\n",
            "Step: 22500, Loss: 275.069580, Accuracy: 0.991600\n",
            "Step: 23000, Loss: 265.508270, Accuracy: 0.991100\n",
            "Step: 23500, Loss: 235.336777, Accuracy: 0.992400\n",
            "Step: 24000, Loss: 269.125153, Accuracy: 0.990800\n",
            "Step: 24500, Loss: 336.728577, Accuracy: 0.991300\n",
            "Step: 25000, Loss: 233.282410, Accuracy: 0.992700\n",
            "Step: 25500, Loss: 268.205872, Accuracy: 0.991700\n",
            "Step: 26000, Loss: 233.668915, Accuracy: 0.991900\n",
            "Step: 26500, Loss: 301.973511, Accuracy: 0.991000\n",
            "Step: 27000, Loss: 233.482727, Accuracy: 0.992300\n",
            "Step: 27500, Loss: 283.969238, Accuracy: 0.991000\n",
            "Step: 28000, Loss: 256.860840, Accuracy: 0.991300\n",
            "Step: 28500, Loss: 279.768555, Accuracy: 0.991900\n",
            "Step: 29000, Loss: 265.617706, Accuracy: 0.992400\n",
            "Step: 29500, Loss: 271.037292, Accuracy: 0.992200\n",
            "Step: 30000, Loss: 274.901581, Accuracy: 0.991700\n",
            "Step: 30500, Loss: 209.162216, Accuracy: 0.993200\n",
            "Step: 31000, Loss: 264.344513, Accuracy: 0.993200\n",
            "Step: 31500, Loss: 256.489227, Accuracy: 0.991800\n",
            "Step: 32000, Loss: 320.586914, Accuracy: 0.990900\n",
            "Step: 32500, Loss: 291.191071, Accuracy: 0.991400\n",
            "Step: 33000, Loss: 341.634521, Accuracy: 0.991000\n",
            "Step: 33500, Loss: 309.563354, Accuracy: 0.991600\n",
            "Step: 34000, Loss: 302.476715, Accuracy: 0.990700\n",
            "Step: 34500, Loss: 326.990265, Accuracy: 0.991000\n",
            "Step: 35000, Loss: 367.459198, Accuracy: 0.991000\n",
            "Step: 35500, Loss: 282.971375, Accuracy: 0.992100\n",
            "Step: 36000, Loss: 332.908508, Accuracy: 0.990900\n",
            "Step: 36500, Loss: 266.129272, Accuracy: 0.992200\n",
            "Step: 37000, Loss: 270.421204, Accuracy: 0.992300\n",
            "Step: 37500, Loss: 287.912628, Accuracy: 0.992200\n",
            "Step: 38000, Loss: 254.612061, Accuracy: 0.993000\n",
            "Step: 38500, Loss: 276.852478, Accuracy: 0.991600\n",
            "Step: 39000, Loss: 291.854706, Accuracy: 0.992400\n",
            "Step: 39500, Loss: 302.115387, Accuracy: 0.992100\n",
            "Step: 40000, Loss: 241.343948, Accuracy: 0.992900\n",
            "Step: 40500, Loss: 293.350128, Accuracy: 0.990600\n",
            "Step: 41000, Loss: 273.425415, Accuracy: 0.991600\n",
            "Step: 41500, Loss: 303.816223, Accuracy: 0.992400\n",
            "Step: 42000, Loss: 281.495422, Accuracy: 0.992900\n",
            "Step: 42500, Loss: 296.450653, Accuracy: 0.992500\n",
            "Step: 43000, Loss: 267.233154, Accuracy: 0.993000\n",
            "Step: 43500, Loss: 285.312988, Accuracy: 0.992700\n",
            "Step: 44000, Loss: 369.535553, Accuracy: 0.990300\n",
            "Step: 44500, Loss: 305.074188, Accuracy: 0.992500\n",
            "Step: 45000, Loss: 265.764832, Accuracy: 0.994000\n",
            "Step: 45500, Loss: 299.286469, Accuracy: 0.992600\n",
            "Step: 46000, Loss: 380.651855, Accuracy: 0.990800\n",
            "Step: 46500, Loss: 304.315948, Accuracy: 0.992100\n",
            "Step: 47000, Loss: 306.948700, Accuracy: 0.992900\n",
            "Step: 47500, Loss: 287.619232, Accuracy: 0.993000\n",
            "Step: 48000, Loss: 292.717743, Accuracy: 0.992400\n",
            "Step: 48500, Loss: 327.755066, Accuracy: 0.991600\n",
            "Step: 49000, Loss: 320.280334, Accuracy: 0.991700\n",
            "Step: 49500, Loss: 293.876984, Accuracy: 0.993200\n",
            "Step: 50000, Loss: 283.091644, Accuracy: 0.993200\n",
            "Step: 50500, Loss: 330.517090, Accuracy: 0.991800\n",
            "Step: 51000, Loss: 276.695099, Accuracy: 0.992400\n",
            "Step: 51500, Loss: 279.414978, Accuracy: 0.992800\n",
            "Step: 52000, Loss: 311.457092, Accuracy: 0.993000\n",
            "Step: 52500, Loss: 298.347717, Accuracy: 0.993000\n",
            "Step: 53000, Loss: 347.047791, Accuracy: 0.992100\n",
            "Step: 53500, Loss: 273.976471, Accuracy: 0.993400\n",
            "Step: 54000, Loss: 339.963837, Accuracy: 0.991900\n",
            "Step: 54500, Loss: 318.857208, Accuracy: 0.993100\n",
            "Step: 55000, Loss: 354.194885, Accuracy: 0.992400\n",
            "Step: 55500, Loss: 276.709839, Accuracy: 0.993700\n",
            "Step: 56000, Loss: 343.384033, Accuracy: 0.992400\n",
            "Step: 56500, Loss: 318.101410, Accuracy: 0.992700\n",
            "Step: 57000, Loss: 351.821381, Accuracy: 0.992200\n",
            "Step: 57500, Loss: 297.362396, Accuracy: 0.993400\n",
            "Step: 58000, Loss: 301.168182, Accuracy: 0.992800\n",
            "Step: 58500, Loss: 339.990540, Accuracy: 0.992300\n",
            "Step: 59000, Loss: 311.642151, Accuracy: 0.992900\n",
            "Step: 59500, Loss: 273.962921, Accuracy: 0.994300\n",
            "Step: 60000, Loss: 323.541382, Accuracy: 0.991900\n",
            "Step: 60500, Loss: 345.434387, Accuracy: 0.992300\n",
            "Step: 61000, Loss: 305.052307, Accuracy: 0.992800\n",
            "Step: 61500, Loss: 353.530396, Accuracy: 0.991800\n",
            "Step: 62000, Loss: 318.795746, Accuracy: 0.992700\n",
            "Step: 62500, Loss: 325.481018, Accuracy: 0.993400\n",
            "Step: 63000, Loss: 368.183594, Accuracy: 0.991800\n",
            "Step: 63500, Loss: 311.383667, Accuracy: 0.993100\n",
            "Step: 64000, Loss: 426.086700, Accuracy: 0.992100\n",
            "Step: 64500, Loss: 339.905151, Accuracy: 0.992300\n",
            "Step: 65000, Loss: 318.604797, Accuracy: 0.992700\n",
            "Step: 65500, Loss: 289.878693, Accuracy: 0.993100\n",
            "Step: 66000, Loss: 298.099792, Accuracy: 0.993700\n",
            "Step: 66500, Loss: 334.910889, Accuracy: 0.992800\n",
            "Step: 67000, Loss: 355.963959, Accuracy: 0.992000\n",
            "Step: 67500, Loss: 314.233459, Accuracy: 0.993000\n",
            "Step: 68000, Loss: 319.664856, Accuracy: 0.992900\n",
            "Step: 68500, Loss: 311.084930, Accuracy: 0.992900\n",
            "Step: 69000, Loss: 308.192383, Accuracy: 0.993200\n",
            "Step: 69500, Loss: 392.279663, Accuracy: 0.991200\n",
            "Step: 70000, Loss: 444.989441, Accuracy: 0.990700\n",
            "Step: 70500, Loss: 319.234039, Accuracy: 0.992900\n",
            "Step: 71000, Loss: 324.456635, Accuracy: 0.993000\n",
            "Step: 71500, Loss: 352.575043, Accuracy: 0.992500\n",
            "Step: 72000, Loss: 289.007568, Accuracy: 0.994000\n",
            "Step: 72500, Loss: 280.097839, Accuracy: 0.994300\n",
            "Step: 73000, Loss: 296.341827, Accuracy: 0.993200\n",
            "Step: 73500, Loss: 306.164825, Accuracy: 0.994000\n",
            "Step: 74000, Loss: 432.593323, Accuracy: 0.989200\n",
            "Step: 74500, Loss: 429.937531, Accuracy: 0.992200\n",
            "Step: 75000, Loss: 320.269989, Accuracy: 0.993600\n",
            "Step: 75500, Loss: 308.232544, Accuracy: 0.993700\n",
            "Step: 76000, Loss: 350.938263, Accuracy: 0.993400\n",
            "Step: 76500, Loss: 338.229004, Accuracy: 0.993100\n",
            "Step: 77000, Loss: 403.575531, Accuracy: 0.992400\n",
            "Step: 77500, Loss: 344.823944, Accuracy: 0.994400\n",
            "Step: 78000, Loss: 315.547028, Accuracy: 0.993600\n",
            "Step: 78500, Loss: 301.382172, Accuracy: 0.993400\n",
            "Step: 79000, Loss: 301.724609, Accuracy: 0.993500\n",
            "Step: 79500, Loss: 340.763641, Accuracy: 0.993600\n",
            "Step: 80000, Loss: 313.002014, Accuracy: 0.993900\n",
            "Step: 80500, Loss: 399.376282, Accuracy: 0.991400\n",
            "Step: 81000, Loss: 329.175934, Accuracy: 0.992700\n",
            "Step: 81500, Loss: 332.763306, Accuracy: 0.993300\n",
            "Step: 82000, Loss: 387.916809, Accuracy: 0.992300\n",
            "Step: 82500, Loss: 317.336884, Accuracy: 0.993100\n",
            "Step: 83000, Loss: 325.437317, Accuracy: 0.993200\n",
            "Step: 83500, Loss: 314.803986, Accuracy: 0.993700\n",
            "Step: 84000, Loss: 352.288483, Accuracy: 0.993300\n",
            "Step: 84500, Loss: 339.544342, Accuracy: 0.993400\n",
            "Step: 85000, Loss: 322.645233, Accuracy: 0.993200\n",
            "Step: 85500, Loss: 317.050476, Accuracy: 0.993900\n",
            "Step: 86000, Loss: 317.963623, Accuracy: 0.993800\n",
            "Step: 86500, Loss: 360.438934, Accuracy: 0.992800\n",
            "Step: 87000, Loss: 317.079407, Accuracy: 0.993600\n",
            "Step: 87500, Loss: 303.770477, Accuracy: 0.993600\n",
            "Step: 88000, Loss: 275.186005, Accuracy: 0.994500\n",
            "Step: 88500, Loss: 316.675781, Accuracy: 0.993100\n",
            "Step: 89000, Loss: 377.255127, Accuracy: 0.992400\n",
            "Step: 89500, Loss: 329.222595, Accuracy: 0.993200\n",
            "Step: 90000, Loss: 365.239075, Accuracy: 0.993000\n",
            "Step: 90500, Loss: 334.150726, Accuracy: 0.993100\n",
            "Step: 91000, Loss: 275.778839, Accuracy: 0.994400\n",
            "Step: 91500, Loss: 324.497711, Accuracy: 0.993400\n",
            "Step: 92000, Loss: 297.855591, Accuracy: 0.992900\n",
            "Step: 92500, Loss: 292.264984, Accuracy: 0.993700\n",
            "Step: 93000, Loss: 303.623810, Accuracy: 0.993400\n",
            "Step: 93500, Loss: 325.638855, Accuracy: 0.993000\n",
            "Step: 94000, Loss: 345.266541, Accuracy: 0.993200\n",
            "Step: 94500, Loss: 384.180420, Accuracy: 0.992800\n",
            "Step: 95000, Loss: 382.973145, Accuracy: 0.992800\n",
            "Step: 95500, Loss: 506.009521, Accuracy: 0.991700\n",
            "Step: 96000, Loss: 378.162445, Accuracy: 0.993000\n",
            "Step: 96500, Loss: 330.393311, Accuracy: 0.993400\n",
            "Step: 97000, Loss: 383.158752, Accuracy: 0.992800\n",
            "Step: 97500, Loss: 446.602661, Accuracy: 0.992600\n",
            "Step: 98000, Loss: 345.590668, Accuracy: 0.992800\n",
            "Step: 98500, Loss: 328.948853, Accuracy: 0.993500\n",
            "Step: 99000, Loss: 330.408600, Accuracy: 0.993600\n",
            "Step: 99500, Loss: 357.105621, Accuracy: 0.992800\n",
            "Step: 100000, Loss: 366.831116, Accuracy: 0.993200\n",
            "Step: 100500, Loss: 386.932037, Accuracy: 0.992400\n",
            "Step: 101000, Loss: 475.187408, Accuracy: 0.992400\n",
            "Step: 101500, Loss: 379.793884, Accuracy: 0.992700\n",
            "Step: 102000, Loss: 386.838745, Accuracy: 0.993400\n",
            "Step: 102500, Loss: 391.490997, Accuracy: 0.992000\n",
            "Step: 103000, Loss: 366.057892, Accuracy: 0.993500\n",
            "Step: 103500, Loss: 317.522858, Accuracy: 0.994000\n",
            "Step: 104000, Loss: 332.403381, Accuracy: 0.993800\n",
            "Step: 104500, Loss: 372.431702, Accuracy: 0.993000\n",
            "Step: 105000, Loss: 362.506531, Accuracy: 0.993600\n",
            "Step: 105500, Loss: 334.190125, Accuracy: 0.993800\n",
            "Step: 106000, Loss: 348.526184, Accuracy: 0.993100\n",
            "Step: 106500, Loss: 327.482269, Accuracy: 0.994000\n",
            "Step: 107000, Loss: 366.313904, Accuracy: 0.993400\n",
            "Step: 107500, Loss: 382.183899, Accuracy: 0.993600\n",
            "Step: 108000, Loss: 410.127747, Accuracy: 0.992100\n",
            "Step: 108500, Loss: 364.450623, Accuracy: 0.993500\n",
            "Step: 109000, Loss: 321.340302, Accuracy: 0.994300\n",
            "Step: 109500, Loss: 433.004333, Accuracy: 0.992400\n",
            "Step: 110000, Loss: 357.727539, Accuracy: 0.993200\n",
            "Step: 110500, Loss: 375.483459, Accuracy: 0.993500\n",
            "Step: 111000, Loss: 375.873444, Accuracy: 0.993200\n",
            "Step: 111500, Loss: 405.640472, Accuracy: 0.992600\n",
            "Step: 112000, Loss: 356.616669, Accuracy: 0.993600\n",
            "Step: 112500, Loss: 379.914124, Accuracy: 0.993300\n",
            "Step: 113000, Loss: 360.666779, Accuracy: 0.993600\n",
            "Step: 113500, Loss: 372.453949, Accuracy: 0.993800\n",
            "Step: 114000, Loss: 366.119843, Accuracy: 0.993700\n",
            "Step: 114500, Loss: 363.072632, Accuracy: 0.993000\n",
            "Step: 115000, Loss: 356.900360, Accuracy: 0.992700\n",
            "Step: 115500, Loss: 353.742615, Accuracy: 0.993100\n",
            "Step: 116000, Loss: 378.002380, Accuracy: 0.993400\n",
            "Step: 116500, Loss: 377.502441, Accuracy: 0.993100\n",
            "Step: 117000, Loss: 392.106262, Accuracy: 0.993500\n",
            "Step: 117500, Loss: 384.586456, Accuracy: 0.993900\n",
            "Step: 118000, Loss: 374.406158, Accuracy: 0.993100\n",
            "Step: 118500, Loss: 344.887939, Accuracy: 0.994100\n",
            "Step: 119000, Loss: 320.381561, Accuracy: 0.993800\n",
            "Step: 119500, Loss: 374.511658, Accuracy: 0.993300\n",
            "Step: 120000, Loss: 358.567902, Accuracy: 0.994200\n",
            "Step: 120500, Loss: 427.020905, Accuracy: 0.993200\n",
            "Step: 121000, Loss: 408.900787, Accuracy: 0.992500\n",
            "Step: 121500, Loss: 412.371826, Accuracy: 0.993200\n",
            "Step: 122000, Loss: 411.520691, Accuracy: 0.993100\n",
            "Step: 122500, Loss: 360.060547, Accuracy: 0.994000\n",
            "Step: 123000, Loss: 366.155334, Accuracy: 0.993600\n",
            "Step: 123500, Loss: 345.527893, Accuracy: 0.994400\n",
            "Step: 124000, Loss: 428.781097, Accuracy: 0.993000\n",
            "Step: 124500, Loss: 421.400970, Accuracy: 0.992200\n",
            "Step: 125000, Loss: 371.819672, Accuracy: 0.993300\n",
            "Step: 125500, Loss: 438.740875, Accuracy: 0.992900\n",
            "Step: 126000, Loss: 356.596252, Accuracy: 0.993500\n",
            "Step: 126500, Loss: 439.260498, Accuracy: 0.992600\n",
            "Step: 127000, Loss: 437.090698, Accuracy: 0.992100\n",
            "Step: 127500, Loss: 350.674561, Accuracy: 0.993600\n",
            "Step: 128000, Loss: 382.114746, Accuracy: 0.993600\n",
            "Step: 128500, Loss: 394.734436, Accuracy: 0.994000\n",
            "Step: 129000, Loss: 376.980835, Accuracy: 0.992900\n",
            "Step: 129500, Loss: 361.547363, Accuracy: 0.993900\n",
            "Step: 130000, Loss: 376.940216, Accuracy: 0.992800\n",
            "Step: 130500, Loss: 349.445953, Accuracy: 0.993100\n",
            "Step: 131000, Loss: 354.498413, Accuracy: 0.993700\n",
            "Step: 131500, Loss: 387.056274, Accuracy: 0.993000\n",
            "Step: 132000, Loss: 371.531250, Accuracy: 0.993900\n",
            "Step: 132500, Loss: 369.856750, Accuracy: 0.993500\n",
            "Step: 133000, Loss: 334.344208, Accuracy: 0.994200\n",
            "Step: 133500, Loss: 357.434509, Accuracy: 0.993700\n",
            "Step: 134000, Loss: 419.834961, Accuracy: 0.993600\n",
            "Step: 134500, Loss: 397.676849, Accuracy: 0.993100\n",
            "Step: 135000, Loss: 371.183472, Accuracy: 0.993600\n",
            "Step: 135500, Loss: 369.842224, Accuracy: 0.994100\n",
            "Step: 136000, Loss: 342.101807, Accuracy: 0.994100\n",
            "Step: 136500, Loss: 379.215820, Accuracy: 0.992800\n",
            "Step: 137000, Loss: 371.173737, Accuracy: 0.993800\n",
            "Step: 137500, Loss: 434.508423, Accuracy: 0.993300\n",
            "Step: 138000, Loss: 409.306671, Accuracy: 0.993800\n",
            "Step: 138500, Loss: 485.831573, Accuracy: 0.992300\n",
            "Step: 139000, Loss: 471.011627, Accuracy: 0.991800\n",
            "Step: 139500, Loss: 428.680756, Accuracy: 0.993100\n",
            "Step: 140000, Loss: 434.119751, Accuracy: 0.992500\n",
            "Step: 140500, Loss: 411.732056, Accuracy: 0.993200\n",
            "Step: 141000, Loss: 432.409851, Accuracy: 0.992700\n",
            "Step: 141500, Loss: 460.909790, Accuracy: 0.992400\n",
            "Step: 142000, Loss: 382.206696, Accuracy: 0.993600\n",
            "Step: 142500, Loss: 446.065735, Accuracy: 0.992700\n",
            "Step: 143000, Loss: 447.020264, Accuracy: 0.992500\n",
            "Step: 143500, Loss: 419.009064, Accuracy: 0.992900\n",
            "Step: 144000, Loss: 480.273651, Accuracy: 0.992700\n",
            "Step: 144500, Loss: 383.138733, Accuracy: 0.994000\n",
            "Step: 145000, Loss: 436.177216, Accuracy: 0.993400\n",
            "Step: 145500, Loss: 525.398682, Accuracy: 0.992100\n",
            "Step: 146000, Loss: 456.866425, Accuracy: 0.993600\n",
            "Step: 146500, Loss: 422.811768, Accuracy: 0.992900\n",
            "Step: 147000, Loss: 431.392670, Accuracy: 0.993700\n",
            "Step: 147500, Loss: 419.372253, Accuracy: 0.993400\n",
            "Step: 148000, Loss: 446.056152, Accuracy: 0.993700\n",
            "Step: 148500, Loss: 449.146606, Accuracy: 0.993700\n",
            "Step: 149000, Loss: 453.084229, Accuracy: 0.993700\n",
            "Step: 149500, Loss: 468.421448, Accuracy: 0.992800\n",
            "Step: 150000, Loss: 474.821838, Accuracy: 0.993100\n",
            "Step: 150500, Loss: 424.914429, Accuracy: 0.993800\n",
            "Step: 151000, Loss: 420.222534, Accuracy: 0.993800\n",
            "Step: 151500, Loss: 440.065247, Accuracy: 0.993700\n",
            "Step: 152000, Loss: 405.952393, Accuracy: 0.994000\n",
            "Step: 152500, Loss: 418.306213, Accuracy: 0.994000\n",
            "Step: 153000, Loss: 450.434052, Accuracy: 0.993800\n",
            "Step: 153500, Loss: 371.392639, Accuracy: 0.994900\n",
            "Step: 154000, Loss: 402.801147, Accuracy: 0.994100\n",
            "Step: 154500, Loss: 376.180969, Accuracy: 0.994100\n",
            "Step: 155000, Loss: 384.539307, Accuracy: 0.994000\n",
            "Step: 155500, Loss: 464.982605, Accuracy: 0.993700\n",
            "Step: 156000, Loss: 486.717865, Accuracy: 0.992800\n",
            "Step: 156500, Loss: 541.698181, Accuracy: 0.993200\n",
            "Step: 157000, Loss: 541.913452, Accuracy: 0.991600\n",
            "Step: 157500, Loss: 423.834076, Accuracy: 0.993700\n",
            "Step: 158000, Loss: 417.785339, Accuracy: 0.994000\n",
            "Step: 158500, Loss: 498.921326, Accuracy: 0.993200\n",
            "Step: 159000, Loss: 441.340118, Accuracy: 0.994300\n",
            "Step: 159500, Loss: 461.256653, Accuracy: 0.993800\n",
            "Step: 160000, Loss: 438.406281, Accuracy: 0.993700\n",
            "Step: 160500, Loss: 471.227905, Accuracy: 0.993400\n",
            "Step: 161000, Loss: 520.878967, Accuracy: 0.993000\n",
            "Step: 161500, Loss: 409.403290, Accuracy: 0.993900\n",
            "Step: 162000, Loss: 458.338226, Accuracy: 0.993800\n",
            "Step: 162500, Loss: 476.258301, Accuracy: 0.993500\n",
            "Step: 163000, Loss: 485.992432, Accuracy: 0.993500\n",
            "Step: 163500, Loss: 564.190186, Accuracy: 0.992300\n",
            "Step: 164000, Loss: 517.984741, Accuracy: 0.993400\n",
            "Step: 164500, Loss: 408.640076, Accuracy: 0.993700\n",
            "Step: 165000, Loss: 423.508942, Accuracy: 0.993300\n",
            "Step: 165500, Loss: 459.793152, Accuracy: 0.993600\n",
            "Step: 166000, Loss: 404.444763, Accuracy: 0.993800\n",
            "Step: 166500, Loss: 418.401550, Accuracy: 0.993900\n",
            "Step: 167000, Loss: 429.748596, Accuracy: 0.994100\n",
            "Step: 167500, Loss: 429.454254, Accuracy: 0.993100\n",
            "Step: 168000, Loss: 470.103149, Accuracy: 0.993600\n",
            "Step: 168500, Loss: 457.675629, Accuracy: 0.993000\n",
            "Step: 169000, Loss: 529.419556, Accuracy: 0.992400\n",
            "Step: 169500, Loss: 484.888367, Accuracy: 0.992600\n",
            "Step: 170000, Loss: 429.030945, Accuracy: 0.993500\n",
            "Step: 170500, Loss: 425.096313, Accuracy: 0.993900\n",
            "Step: 171000, Loss: 435.493927, Accuracy: 0.993900\n",
            "Step: 171500, Loss: 424.671356, Accuracy: 0.994200\n",
            "Step: 172000, Loss: 464.566620, Accuracy: 0.993600\n",
            "Step: 172500, Loss: 438.568176, Accuracy: 0.993700\n",
            "Step: 173000, Loss: 400.559204, Accuracy: 0.993800\n",
            "Step: 173500, Loss: 434.182312, Accuracy: 0.993700\n",
            "Step: 174000, Loss: 491.332336, Accuracy: 0.992500\n",
            "Step: 174500, Loss: 434.115753, Accuracy: 0.994000\n",
            "Step: 175000, Loss: 411.362061, Accuracy: 0.994100\n",
            "Step: 175500, Loss: 414.330994, Accuracy: 0.994300\n",
            "Step: 176000, Loss: 395.723663, Accuracy: 0.993600\n",
            "Step: 176500, Loss: 415.266846, Accuracy: 0.993800\n",
            "Step: 177000, Loss: 422.710083, Accuracy: 0.993900\n",
            "Step: 177500, Loss: 437.707428, Accuracy: 0.993600\n",
            "Step: 178000, Loss: 375.619781, Accuracy: 0.994200\n",
            "Step: 178500, Loss: 425.436493, Accuracy: 0.994300\n",
            "Step: 179000, Loss: 467.806580, Accuracy: 0.994400\n",
            "Step: 179500, Loss: 371.505005, Accuracy: 0.994400\n",
            "Step: 180000, Loss: 439.430481, Accuracy: 0.993200\n",
            "Step: 180500, Loss: 634.127808, Accuracy: 0.992300\n",
            "Step: 181000, Loss: 510.610046, Accuracy: 0.992400\n",
            "Step: 181500, Loss: 417.127319, Accuracy: 0.993700\n",
            "Step: 182000, Loss: 474.171906, Accuracy: 0.994200\n",
            "Step: 182500, Loss: 428.114166, Accuracy: 0.994700\n",
            "Step: 183000, Loss: 452.180450, Accuracy: 0.993400\n",
            "Step: 183500, Loss: 449.961273, Accuracy: 0.993700\n",
            "Step: 184000, Loss: 450.159119, Accuracy: 0.994200\n",
            "Step: 184500, Loss: 474.837769, Accuracy: 0.993600\n",
            "Step: 185000, Loss: 437.921570, Accuracy: 0.994500\n",
            "Step: 185500, Loss: 420.991577, Accuracy: 0.993700\n",
            "Step: 186000, Loss: 486.580292, Accuracy: 0.993500\n",
            "Step: 186500, Loss: 536.892639, Accuracy: 0.993400\n",
            "Step: 187000, Loss: 459.541748, Accuracy: 0.993400\n",
            "Step: 187500, Loss: 424.132111, Accuracy: 0.993700\n",
            "Step: 188000, Loss: 484.622711, Accuracy: 0.993700\n",
            "Step: 188500, Loss: 398.121399, Accuracy: 0.994300\n",
            "Step: 189000, Loss: 461.749054, Accuracy: 0.993900\n",
            "Step: 189500, Loss: 476.985291, Accuracy: 0.994000\n",
            "Step: 190000, Loss: 508.133392, Accuracy: 0.993700\n",
            "Step: 190500, Loss: 492.556122, Accuracy: 0.993900\n",
            "Step: 191000, Loss: 445.738525, Accuracy: 0.993600\n",
            "Step: 191500, Loss: 517.909790, Accuracy: 0.993800\n",
            "Step: 192000, Loss: 478.239899, Accuracy: 0.993400\n",
            "Step: 192500, Loss: 457.975281, Accuracy: 0.994200\n",
            "Step: 193000, Loss: 411.654633, Accuracy: 0.993900\n",
            "Step: 193500, Loss: 491.526886, Accuracy: 0.993800\n",
            "Step: 194000, Loss: 515.909363, Accuracy: 0.993300\n",
            "Step: 194500, Loss: 454.680908, Accuracy: 0.993700\n",
            "Step: 195000, Loss: 467.551758, Accuracy: 0.993500\n",
            "Step: 195500, Loss: 421.523193, Accuracy: 0.993900\n",
            "Step: 196000, Loss: 471.371765, Accuracy: 0.994400\n",
            "Step: 196500, Loss: 508.542480, Accuracy: 0.993400\n",
            "Step: 197000, Loss: 515.282410, Accuracy: 0.992800\n",
            "Step: 197500, Loss: 502.044373, Accuracy: 0.993500\n",
            "Step: 198000, Loss: 449.864624, Accuracy: 0.993900\n",
            "Step: 198500, Loss: 479.380920, Accuracy: 0.993300\n",
            "Step: 199000, Loss: 485.363129, Accuracy: 0.993100\n",
            "Step: 199500, Loss: 489.861572, Accuracy: 0.992500\n",
            "Step: 200000, Loss: 474.134552, Accuracy: 0.993000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl07NB3-RRPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls cnn_session*"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}